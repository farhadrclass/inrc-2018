{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DNNs on Loihi\n",
    "\n",
    "This notebook provides a minimal example of a keyword spotter trained offline and then run on Loihi. To keep things short for this tutorial, we'll implement a single layer of 200 neurons on the chip, and use this network to predict character transcriptions of audio signals. First we'll import some utility functions for initializing weights and for converting a nengo simulation output to text characters.\n",
    "\n",
    "The purpose of this example illustrative - since we're keeping things short and simple, we're not aiming to get a high-accuracy model; rather, we're trying to summarize the key steps in the process of building such a model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/peterblouw/anaconda/envs/iceland/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import nengo\n",
    "import nengo_dl\n",
    "import nengo_loihi\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "from utils import predict_text, ce_loss, weight_init, create_stream\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Initial Setup\n",
    "\n",
    "First, we set some high level parametes and load the data we'll be training the model on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp_dim = 390\n",
    "out_dim = 29\n",
    "\n",
    "n_neurons = 200\n",
    "max_rate = 250\n",
    "amplitude = 1 / max_rate\n",
    "\n",
    "lifs = nengo.LIF(tau_rc=0.02, tau_ref=0.002, amplitude=amplitude)\n",
    "\n",
    "with open('./data/data.pkl', 'rb') as pfile:\n",
    "    train_data = pickle.load(pfile)\n",
    "\n",
    "with open('./data/stream.pkl', 'rb') as pfile:\n",
    "    test_data = pickle.load(pfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Define the Network\n",
    "\n",
    "This is very similar to how you typically use Nengo: create collections of neurons, then connect them to input nodes to provide data to the network. We'll just add a couple of flags to configure how the network is handled by Nengo DL:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with nengo.Network() as net:\n",
    "    nengo_dl.configure_settings(trainable=True)\n",
    "    net.config[nengo.Connection].synapse = None\n",
    "    net.config[nengo.Ensemble].max_rates = nengo.dists.Choice([max_rate])\n",
    "    net.config[nengo.Ensemble].intercepts = nengo.dists.Choice([0])\n",
    "\n",
    "    inp = nengo.Node(np.zeros(inp_dim))\n",
    "    ens = nengo.Ensemble(n_neurons=n_neurons, dimensions=1, neuron_type=lifs)\n",
    "    out = nengo.Node(size_in=out_dim)\n",
    "\n",
    "    conn_a = nengo.Connection(\n",
    "        inp, ens.neurons, transform=weight_init(shape=(n_neurons, inp_dim)))\n",
    "    \n",
    "    conn_b = nengo.Connection(\n",
    "        ens.neurons, out, transform=weight_init(shape=(out_dim, n_neurons)))\n",
    "\n",
    "    probe = nengo.Probe(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Train the Network\n",
    "\n",
    "Now we can take our constructed model and use Nengo DL to optimize its parameters with some minimal tweaks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Build finished in 0:00:00                                                      \n",
      "Optimization finished in 0:00:00                                               \n",
      "Construction finished in 0:00:00                                               \n",
      "WARNING:tensorflow:From /Users/peterblouw/abr/inrc-2018/utils.py:11: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See @{tf.nn.softmax_cross_entropy_with_logits_v2}.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/peterblouw/abr/inrc-2018/utils.py:11: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See @{tf.nn.softmax_cross_entropy_with_logits_v2}.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training finished in 0:00:16 (loss: 11.7059)                                   \n"
     ]
    }
   ],
   "source": [
    "# create a Nengo DL simulator and set the minibatch size\n",
    "with nengo_dl.Simulator(net, minibatch_size=100) as sim:\n",
    "\n",
    "    # define an optimizer\n",
    "    optimizer = tf.train.RMSPropOptimizer(0.001)\n",
    "    \n",
    "    # specify inputs and target\n",
    "    inputs = {inp: train_data['inp']}\n",
    "    targets = {probe: train_data['out']}\n",
    "    \n",
    "    # define a loss function\n",
    "    objective = {probe: ce_loss}\n",
    "\n",
    "    # optimize the model parameters\n",
    "    sim.train(\n",
    "        inputs, targets, optimizer, n_epochs=30, objective=objective)\n",
    "    \n",
    "    # collect the parameters to port to loihi\n",
    "    params = sim.get_nengo_params([ens, conn_a, conn_b])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Rebuild the Network for Loihi\n",
    "\n",
    "Now, we reconstruct the network, initializing the ensemble and connections with the trained parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with nengo.Network() as loihi_net:\n",
    "    loihi_net.config[nengo.Connection].synapse = None\n",
    "\n",
    "    inp = nengo.Node(np.zeros(inp_dim))\n",
    "    ens = nengo.Ensemble(n_neurons=n_neurons, dimensions=1, **params[0])\n",
    "    out = nengo.Node(size_in=out_dim)\n",
    "\n",
    "    conn_a = nengo.Connection(\n",
    "        inp, ens.neurons, transform=params[1]['transform'] * amplitude)\n",
    "\n",
    "    conn_b = nengo.Connection(\n",
    "        ens.neurons, out, transform=params[2]['transform'] * amplitude)\n",
    "\n",
    "    probe = nengo.Probe(out, synapse=0.01)\n",
    "    loihi_net.inp = inp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Run the Keywork Spotter on Loihi\n",
    "\n",
    "Here we'll use an emulator for the chip. Once the inputs are setup, it's just one line of code to run the keyword spotter on Loihi!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct Text: aloha\n",
      "Predicted Text: aloaha\n",
      "\n",
      "Correct Text: aloha\n",
      "Predicted Text: aloha\n",
      "\n",
      "Correct Text: aloha\n",
      "Predicted Text: aloha\n",
      "\n",
      "Correct Text: aloha\n",
      "Predicted Text: aloha\n",
      "\n",
      "Correct Text: aloha\n",
      "Predicted Text: ala\n",
      "\n",
      "Correct Text: aloha\n",
      "Predicted Text: alaha\n",
      "\n",
      "Correct Text: aloha\n",
      "Predicted Text: al\n",
      "\n",
      "Correct Text: aloha\n",
      "Predicted Text: aloaha\n",
      "\n",
      "Correct Text: aloha\n",
      "Predicted Text: alaoha\n",
      "\n",
      "Correct Text: aloha\n",
      "Predicted Text: ala\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for features, text in test_data[:10]:\n",
    "    n_steps = features.shape[0]\n",
    "    loihi_net.inp.output = create_stream(features)\n",
    "\n",
    "    sim = nengo_loihi.Simulator(loihi_net, target='sim', precompute=True)\n",
    "\n",
    "    with sim:\n",
    "        sim.run_steps(n_steps)\n",
    "        prediction = predict_text(sim, probe, n_steps)\n",
    "        print('Correct Text: %s' % text)\n",
    "        print('Predicted Text: %s' % prediction)\n",
    "        print('')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Though this model is not particularly performant, we can build more sophisticated models that perform comparably to a standard DNN implementation in Tensorflow!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
